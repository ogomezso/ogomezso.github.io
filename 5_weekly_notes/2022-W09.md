# 2022-02-28


##### status: #daily_note 

# Tasks

#todo 
- [x] Provide Spring App image to Allianz
- [x] Provide Helm Chart or CR to deploy it
- [x] Runbook

# Work Notes

--- 
### 09:55

### Status: #fleeting_note
### Tags: [[replicator_monitoring]] [[replicator]] 
### References

### Notes:

#### Install replicator on EKS with Helm Charts

```ad-note
title: Helm Charts

Install helm Charts repo
~~~bash

helm repo add cp-helm-charts https://confluentinc.github.io/cp-helm-charts/
~~~~
Check chart versions on the repo

~~~bash
helm search repo -l cp-helm-charts
~~~

```

```ad-note
title: Namespace

~~~yaml
kind: Namespace
apiVersion: v1
metadata:
  name: allianz
  labels:
    name: confluent
~~~

```

##### Create Namespace

~~~bash
kubectl apply namespace.yaml
~~~



#### EKS Creation

##### Cluster Creation
```ad-warning

If create like this no node group added

~~~bash

eksctl create cluster --name ogomez-eks -f eks-config.yaml
~~~~

```

```ad-error
~~~yaml

apiVersion: eksctl.io/v1alpha5 
kind: ClusterConfig 
metadata: 
  name: ogomez-cluster
  region: us-west-2 
vpc: 
  id: "vpc-09074923ca2d430cb" 
  subnets: 
	public: 
	  eu-west-1a: 
	    id: "subnet-0332cdc15bd0647d3" 
	  eu-west-1b: 
		id: "subnet-023900fa18b9e9802" 
	  eu-west-1c: 
		id: "subnet-04d93b58102b1aca6" 
nodeGroups: 
  - name: ogomez-eks-ng
~~~
```

```ad-note
title: kubectl config

~~~bash
eksctl utils write-kubeconfig --cluster allianz_demo
~~~

```

---
--- 
### 17:38

### Status: #fleeting_note
### Tags: [[replicator_monitoring]] [[replicator]] [[EKS]]

### References
[{Confluent Docs}Replicator Monitoring](https://docs.confluent.io/platform/current/multi-dc-deployments/replicator/replicator-monitoring.html)
#repos [cp-helm-charts](https://github.com/confluentinc/cp-helm-charts)

### Notes:

#### EKS Creation

```ad-success
title: eks-config.yaml

~~~yaml

apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: ogomez-eks-cluster
  region: eu-west-1

vpc:
  id: "vpc-09074923ca2d430cb" 
  subnets:
    private:
      eu-west-1a: { id: subnet-0332cdc15bd0647d3 }
      eu-west-1b: { id: subnet-023900fa18b9e9802 }
      eu-west-1c: { id: subnet-04d93b58102b1aca6 }
nodeGroups:
  - name: ogomez-eks-workers
    labels: { role: workers }
    instanceType: m5.xlarge
    desiredCapacity: 10
    privateNetworking: true
	
~~~

```

~~~bash
eksctl create cluster -f eks-config.yaml -v 4
~~~

```ad-tip

private network is mandatory (public one optional)
cloudformation get all the events of the creation

```

#### Replicator

```ad-error

error conneting putting to work replicator onhelm chart pointing to my msk



```

---
---

# Meeting Notes
# 2022-02-28 09:15 - Sync Call

^0ed583

##### status: #meeting_note
 ##### Tags: [[ccloud_migration]]

### Client: 
[[Allianz]]
### Topic:
### Attendees:
- Marcus Barcelos: Allianz Operator
* Dominique Ronde
* Nakul Barcelos
* Mark Schulze
* Martin Nasser

 
### Notes:

- placeholders to env for the config
- config maps and secrets for spring
- prometheus / grafana with CCloud exporter perhaps migrate it to Metrics API
- Consumer Lag Dashboard!
- No Operator! Helm Charts
- Kafka Connect with Replicator. Replicator Config needed!
- Environment variables for Spring Apps.
- Offset translation by command
- migrate apps will be one off stopping first. Downtime is allowed

----

# 2022-03-02


##### status: #daily_note 

# Tasks

#todo 
- [x] Check Helm Charts

# Work Notes

# Meeting Notes

# 2022-03-02 08:30 - 17:00
##### status: #meeting_note
##### Tags: [[replicator]] [[ccloud_migration]]

### Client:
[[Allianz]]
### Topic:
PREPROD MSK Migration
### Attendees:
* Nakul Mishra
* Oscar Gómez
* Marcus Barcelos
### Notes:

--- 
### 8:30

```ad-error
title: Monitoring

We tried to put in place monitoring with the dashboards we already provide (see Replicator Monitoring Guide).

We are not able to do so because Connect Instance are not exposing metrics.

```

```ad-error
title: Operator

We try to install through CFK Operator, but we don't have the certificates to setup properly the connections, so we avoid it.

We gonna check their Helm charts

```
Helm Chart Provided:

~~~yaml
aws:
  rolearn: 695964746098:role/oidc-role-kafka-connect

openshift: false
stage: "preprod"
dockerImage: "693773657855.dkr.ecr.eu-central-1.amazonaws.com/azdir/kafka-connect"
deployDomain: "allianzdirect-k8s-preprod.ec1.aws.aztec.cloud.allianz"

replicaCount: 2
kafkaOptions: "-Djava.security.auth.login.config=/etc/kafka-connect/basic-auth/config/KafkaConnectJAAS"
connectDebug: "y"

overrideGroupId: "kafka-connect-preprod"

configurationOverridesMsk:
  "plugin.path": "/usr/share/java,/usr/share/confluent-hub-components,/etc,/usr/share/confluent-hub-components/confluentinc-kafka-connect-replicator/lib/*"
  "key.converter": "org.apache.kafka.connect.storage.StringConverter"
  "key.converter.schemas.enable": false
  "value.converter": "io.confluent.connect.avro.AvroConverter"
  "value.converter.schemas.enable": true
  "config.storage.replication.factor": "3"
  "offset.storage.replication.factor": "3"
  "status.storage.replication.factor": "3"
  "security.protocol": "SSL"
  "consumer.security.protocol": "SSL"
  "consumer.key.deserializer": "org.apache.kafka.common.serialization.ByteArrayDeserializer"
  "consumer.value.deserializer": "org.apache.kafka.common.serialization.ByteArrayDeserializer"
  "producer.security.protocol": "SSL"
  "producer.key.serializer": "org.apache.kafka.common.serialization.ByteArraySerializer"
  "producer.value.serializer": "org.apache.kafka.common.serialization.ByteArraySerializer"
  "config.providers": "file"
  "config.providers.file.class": "org.apache.kafka.common.config.provider.FileConfigProvider"
  "rest.extension.classes" : "org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension,io.confluent.connect.replicator.monitoring.ReplicatorMonitoringExtension"
  "rest.advertised.host.name" : "kafka-connect.allianzdirect-k8s-preprod.ec1.aws.aztec.cloud.allianz"
~~~

```ad-success

Helm chart checked and figure out they are missing the prometheus exporter sidecar part.

~~~yaml
prometheus:
  ## JMX Exporter Configuration
  ## ref: [https://github.com/prometheus/jmx_exporter](https://github.com/prometheus/jmx_exporter)
  jmx:
    enabled: true
    image: solsson/kafka-prometheus-jmx-exporter@sha256
    imageTag: 6f82e2b0464f50da8104acd7363fb9b995001ddff77d248379f8788e78946143
    imagePullPolicy: IfNotPresent
    port: 5556

    ## Resources configuration for the JMX exporter container.
    ## See the `resources` documentation above for details.
    resources: {}
~~~

Working on a my cluster. 

Provided to Marcus that gonna put in place by the end of the day (will be effective tomorrow)

```


---

--- 
### 10:30

```ad-info

1 Connect cluster deployed (2 pods)

Replicator Connector deployed on it.

~~~json
{
  "name": "replicator-3",
  "config": {
    "connector.class": "io.confluent.connect.replicator.ReplicatorSourceConnector",
    "dest.kafka.sasl.mechanism": "PLAIN",
    "dest.kafka.ssl.endpoint.identification.algorithm": "https",
    "src.kafka.ssl.key.password": "PASSWORD",
    "topic.whitelist": <bunch of topics>,
    "offset.translator.tasks.max": "0",
    "producer.override.security.protocol": "SASL_SSL",
    "dest.topic.replication.factor": "3",
    "confluent.license": "${file:/etc/kafka-connect/secrets/connectors_passwords.properties:confluent-license}",
    "confluent.topic.ssl.keystore.location": "${file:/etc/kafka-connect/kafka-connect.properties:ssl.keystore.location}",
    "dest.kafka.security.protocol": "SASL_SSL",
    "src.kafka.security.protocol": "SSL",
    "topic.config.sync": "true",
    "confluent.topic.ssl.keystore.type": "PKCS12",
    "dest.kafka.retry.backoff.ms": "500",
    "confluent.topic.replication.factor": "3",
    "producer.override.request.timeout.ms": "20000",
    "src.kafka.ssl.keystore.location": "/etc/kafka-connect/cert/keystore.jks",
    "value.converter": "io.confluent.connect.replicator.util.ByteArrayConverter",
    "key.converter": "io.confluent.connect.replicator.util.ByteArrayConverter",
    "producer.override.retry.backoff.ms": "500",
    "offset.timestamps.commit": "false",
    "producer.override.bootstrap.servers": "lkc-m30yw-lg1x1.eu-central-1.aws.glb.confluent.cloud:9092",
    "confluent.topic.bootstrap.servers": "${file:/etc/kafka-connect/kafka-connect.properties:bootstrap.servers}",
    "dest.kafka.request.timeout.ms": "20000",
    "header.converter": "io.confluent.connect.replicator.util.ByteArrayConverter",
    "src.kafka.sasl.mechanism": "PLAIN",
    "confluent.topic.ssl.keystore.password": "${file:/etc/kafka-connect/kafka-connect.properties:ssl.key.password}",
    "src.consumer.group.id": "replicator-2",
    "dest.kafka.bootstrap.servers": "lkc-m30yw-lg1x1.eu-central-1.aws.glb.confluent.cloud:9092",
    "confluent.topic.security.protocol": "${file:/etc/kafka-connect/kafka-connect.properties:security.protocol}",
    "name": "replicator-3",
    "producer.override.sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"I4VRUB75JCZ37G4T\" password=\"PASSWORD\";",
    "topic.auto.create": "true",
    "producer.override.sasl.mechanism": "PLAIN",
    "provenance.header.enable": "true",
    "dest.kafka.sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"I4VRUB75JCZ37G4T\" password=\"PASSWORD\";",
    "producer.override.ssl.endpoint.identification.algorithm": "https",
    "src.kafka.bootstrap.servers": "SSL://b-1.kafka-pre-prod.0djbl4.c4.kafka.eu-central-1.amazonaws.com:9094,SSL://b-2.kafka-pre-prod.0djbl4.c4.kafka.eu-central-1.amazonaws.com:9094,SSL://b-3.kafka-pre-prod.0djbl4.c4.kafka.eu-central-1.amazonaws.com:9094,SSL://b-4.kafka-pre-prod.0djbl4.c4.kafka.eu-central-1.amazonaws.com:9094,SSL://b-5.kafka-pre-prod.0djbl4.c4.kafka.eu-central-1.amazonaws.com:9094,SSL://b-6.kafka-pre-prod.0djbl4.c4.kafka.eu-central-1.amazonaws.com:9094",
    "src.kafka.ssl.keystore.password": "PASSWORD"
}
~~~

**700 Mill Messages** over **3k+ Partitions**

All topics migrated except:  optr*, __consumer_offset, __amazon, __mm2, __connect_offset, __connect_status


```

```ad-warning

Too many work for 1 instance of replicator, on top of that we have just 1 task configured.

We recommend the approach of having more instances of Replicator (on specific cluster) with specific resources for a group of topics. This will improve:

1. Resilience: In case of failure we don't have to figure out what was wrong on 3k partitions just in a few ones beside of the RTO on that case.
2. Throughput will increase massively

```


```ad-success
title: Replicator Test

Test topic replication without problem:

**

Input-topic : migration-march-1(MSK)

Output:  migration-march-1 (CCloud)

Actual : 10_000

Expected: 10_000

  

Push : 100_000

Expected: 110_000 (100_000 + prev 10_000)

**
```

---

````ad-summary
title: Topic Migration


Migration Starts with 1 Task so as expected we moving slow (not quantified)

Decide to move to 5 task and 5 pods. **Still Slow**

Some disccusion happens about [task rebalancing](https://docs.confluent.io/platform/current/connect/concepts.html#task-rebalancing).

On this point marcus decide to kill some pods and just experiment what happens with the rebalancing.

We decide to move to **30 task with 30 pods**. Reabalancing its happenning on the process.

Now we got a decent thorughput

````

```ad-success


Migration Completed:

**

MSK cluster : 700Million+ messages

CCloud : 40Million messages

**

Looking into the skip topics we realize that just \_consumer\_offset have more than half million. SO the numbers match

```
```ad-warning

We found some duplicates on the migrated topics (at leat 1 serious one with 6x more messages). We can explain that due the killing pods (like a kill -9), rebalancing, network issues.

But on top of that we emphatize the fact of not having this kind of behaivour on a distributed system is almost impossible, and the fact of the *at least one* semantic as default one in Kafka.

```

```ad-summary
title: Test App Migration

We choose a *Spring Cloud App* (producer and consumer in one) to check the migration

We go without any preparation just change bootstrap endpoints and credential on al K8s Map and restart.

```

```ad-error

App is able to produce but not to consume.

We found a error on consumer group permission.

Other environments seems to work properly

```

```ad-hint

On other environments they are running with **user account** credentials that grant permission to al resources.

On PREPROD we run with a **service user** so we need to grant permission properly.

Marcus decide to create a new account user for prepod (**we don't recommend this for PROD**).

After that all working.

```

### Improvement Room and reminders

 #todo
- [x] Provide kcat and ccloud cheatsheet we struggle a lot of time finding and sending to them the commands
- [x] Discuss about topic naming convention (our life can me much more easier)
- [x] Provide runbook about component install with [[cfk]] (I already have one). Push them back from deprecated helmcharts

### Day Recap 

- [x] topics fully migrated
- [x] ACLs put in place
- [x] Test Java App On migrated topic
 
### Next Steps

- [x] Fresh Start on new Cluster$^1$
	- [x] Provision new cluster
	- [x] Create Accounts and new API Keys
- [x] {Nice to Have} Monitoring on the Helm Chart
- [x] Start Replicator with the agreed configuration
- [x] Migrate Topics on the whitelist
- [x] Migrate Producers/Consumers
- [x] Migrate Kstream apps
- [x] Migrate Connectors
- [x] Cleanup

```ad-warning
title: 1

We Expect that Marcus will do some work in advance so tomorrow we can start from the topic migration or even from the Apps but still start from scratch on a new cluster leaves us with few time to complete all the tasks 

```

# 2022-03-03


##### status: #daily_note 

# Tasks

#todo 
- [x] Replicator Monitoring
- [x] Fresh Cluster Provision
- [x] Create Accounts and API Key
- [x] Create Replicator
- [x] Migrate Data
- [x] Migrate Producers
- [x] Migrate Consumers
- [x] Migrate Streams
- [x] Migrate Connect
- [x] Clean Up

# Work Notes

# Meeting Notes

# 2022-03-03 08:36
##### status: #meeting_note
##### Tags:

### Client:
[[Allianz]]

### Topic:
[[ccloud_migration]]

### Attendees:
* Nakul Mishra
* Oscar Gómez Soriano 
* Marcus Barcelos
### Notes:

--- 
### 08:42

### Status: #fleeting_note
### Tags: [[replicator_monitoring]]
### References

### Notes:

```ad-error

Scrapping fails on the scrap. We don't have the same metrics.

Just grab the rules from JMX examples and paste on the helm chart jmx-configmap.yaml

```

~~~
{{- if and .Values.prometheus.jmx.enabled }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "cp-kafka-connect.fullname" . }}-jmx-configmap
  labels:
    app: {{ template "cp-kafka-connect.name" . }}
    chart: {{ template "cp-kafka-connect.chart" . }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
data:
  jmx-kafka-connect-prometheus.yml: |+
    jmxUrl: service:jmx:rmi:///jndi/[rmi://localhost](rmi://localhost):{{ .Values.jmx.port }}/jmxrmi
    lowercaseOutputName: true
    lowercaseOutputLabelNames: true
    ssl: false
    whitelistObjectNames:
    - kafka.connect:type=app-info,client-id=*
    - kafka.connect:type=connect-worker-rebalance-metrics
    - kafka.connect:type=connect-coordinator-metrics,*
    - kafka.connect:type=connect-metrics,*
    - kafka.connect:type=connect-worker-metrics
    - kafka.connect:type=connect-worker-metrics,*
    - kafka.connect:type=connector-metrics,*
    - kafka.connect:type=*-task-metrics,*
    - kafka.connect:type=task-error-metrics,*
    - confluent.replicator:type=confluent-replicator-task-metrics,*
    - "kafka.consumer:*"
    - "kafka.producer:*"
    blacklistObjectNames:
    - "kafka.admin.client:*"
    - "kafka.consumer:type=*,id=*"
    - "kafka.producer:type=*,id=*"
    - "kafka.producer:client-id=confluent.monitoring*,*"
    - "kafka.*:type=kafka-metrics-count,*"
    rules:
      - pattern: "kafka.connect<type=app-info, client-id=(.+)><>(.+): (.+)"
        name: "kafka_connect_app_info"
        value: 1
        labels:
          client-id: "$1"
          $2: "$3"
        type: UNTYPED
      - pattern: "kafka.connect<type=connect-worker-rebalance-metrics><>([^:]+)"
        name: "kafka_connect_connect_worker_rebalance_metrics_$1"
      - pattern: "kafka.connect<type=(.+), client-id=(.+)><>([^:]+)"
        name: kafka_connect_$1_$3
        labels:
          client_id: $2
      - pattern: "kafka.connect<type=connect-worker-metrics><>([^:]+)"
        name: kafka_connect_connect_worker_metrics_$1
        labels:
          connector: "aggregate"
      - pattern: "kafka.connect<type=connect-worker-metrics, connector=(.+)><>([^:]+)"
        name: kafka_connect_connect_worker_metrics_$2
        labels:
          connector: $1
      - pattern: "kafka.connect<type=connector-metrics, connector=(.+)><>(.+): (.+)"
        value: 1
        name: kafka_connect_connector_metrics
        labels:
          connector: $1
          $2: $3
        type: UNTYPED
      - pattern: "kafka.connect<type=(.+)-task-metrics, connector=(.+), task=(\\d+)><>(.+): (.+)"
        name: kafka_connect_$1_task_metrics_$4
        labels:
          connector: "$2"
          task: "$3"
      - pattern: "kafka.connect<type=task-error-metrics, connector=(.+), task=(\\d+)><>([^:]+)"
        name: kafka_connect_task_error_metrics_$3
        labels:
          connector: "$1"
          task: "$2"
      - pattern: "confluent.replicator<type=confluent-replicator-task-metrics, confluent-replicator-(.*)=(.+), confluent-replicator-(.+)=(.+), confluent-replicator-(.+)=(.+), confluent-replicator-(.+)=(.+)><>confluent-replicator-task-topic-partition-(.*): (.*)"
        name: confluent_replicator_task_metrics_$9
        labels:
          $1: "$2"
          $3: "$4"
          $5: "$6"
          $7: "$8"
      - pattern: "confluent.replicator<type=confluent-replicator-task-metrics, confluent-replicator-(.*)=(.+), confluent-replicator-(.+)=(.+), confluent-replicator-(.+)=(.+), confluent-replicator-(.+)=(.+)><>(confluent-replicator-destination-cluster|confluent-replicator-source-cluster|confluent-replicator-destination-topic-name): (.*)"
        name: confluent_replicator_task_metrics_info
        value: 1
        labels:
          $1: "$2"
          $3: "$4"
          $5: "$6"
          $7: "$8"
          $9: "$10"
      # "kafka.consumer:type=app-info,client-id=*"
      # "kafka.producer:type=app-info,client-id=*"
      - pattern: "kafka.(.+)<type=app-info, client-id=(.+)><>(.+): (.+)"
        value: 1
        name: kafka_$1_app_info
        labels:
          client_type: $1
          client_id: $2
          $3: $4
        type: UNTYPED
      # "kafka.consumer:type=consumer-metrics,client-id=*, protocol=*, cipher=*"
      # "kafka.consumer:type=type=consumer-fetch-manager-metrics,client-id=*, topic=*, partition=*"
      # "kafka.producer:type=producer-metrics,client-id=*, protocol=*, cipher=*"
      - pattern: "kafka.(.+)<type=(.+), (.+)=(.+), (.+)=(.+), (.+)=(.+)><>(.+):"
        name: kafka_$1_$2_$9
        type: GAUGE
        labels:
          client_type: $1
          $3: "$4"
          $5: "$6"
          $7: "$8"
      # "kafka.consumer:type=consumer-node-metrics,client-id=*, node-id=*"
      # "kafka.consumer:type=consumer-fetch-manager-metrics,client-id=*, topic=*"
      # "kafka.producer:type=producer-node-metrics,client-id=*, node-id=*"
      # "kafka.producer:type=producer-topic-metrics,client-id=*, topic=*"
      - pattern: "kafka.(.+)<type=(.+), (.+)=(.+), (.+)=(.+)><>(.+):"
        name: kafka_$1_$2_$7
        type: GAUGE
        labels:
          client_type: $1
          $3: "$4"
          $5: "$6"
      # "kafka.consumer:type=consumer-fetch-manager-metrics,client-id=*"
      # "kafka.consumer:type=consumer-metrics,client-id=*"
      # "kafka.producer:type=producer-metrics,client-id=*"
      - pattern: "kafka.(.+)<type=(.+), (.+)=(.+)><>(.+):"
        name: kafka_$1_$2_$5
        type: GAUGE
        labels:
          client_type: $1
          $3: "$4"
      - pattern: "kafka.(.+)<type=(.+)><>(.+):"
        name: kafka_$1_$2_$3
        labels:
          client_type: $1
{{- end }}
~~~


---

--- 
### 10:19

### Status: #fleeting_note
### Tags:
[[replicator_monitoring]]
### References

### Notes:

```ad-success

after tweak dashboards and the scrape rules we have the monitoring in place

```

```ad-info

Go from the scratch with fresh cluster

```


Service Accounts are in place:

![[Pasted image 20220303102703.png]]
---

--- 
### 10:46

### Status: #fleeting_note
### Tags:
[[replicator_config]]

### References

### Notes:

```ad-error

Error when trying to delete the replicator connector.

seems to be a problem with the leader resolution with the rest advertised lister, we have 30 instances
```
```ad-error

{“error_code”:500,“message”:“Error trying to forward REST request: Error trying to forward REST request: Cannot complete request because of a conflicting operation (e.g. worker rebalance)“}

```

```ad-warning

The exception seems no to be ok, there is not rebalancing in process is matter of listeners

```



---
--- 
### 11:47

### Status: #fleeting_note
### Tags:
[[replicator_config]]
### References

### Notes:

```ad-success

scale down replicator to 1 and deletion succeed

```
```ad-error

When scale to 5 the problem again there

```
```ad-info
we figure out that the rest advertided host port config were missing that explain the timeout errors.

rebalance in take place. **it tooks 4min**.
```
```ad-warning

We found errors on the producer config on the replicator

we also found that in some point replication starts and we need to delete all the topics on CCLOUD

We also need to rename replicator just to start the replication from the very beginning

```

---


--- 
### 13:10

### Status: #fleeting_note
### Tags:
[[replicator_config]]
### References

### Notes:

```ad-note

We start to replicate

```

```ad-error

We are not able to replicate from the beginning because of a misconfiguration. They are overriding *src.consumer.groupid* so consumer group is not recreated even with a new connector name.
```

---
--- 
### 16:59

### Status: #fleeting_note
### Tags:
[[ccloud_migration]] 
### References

### Notes:

```ad-success

Replication finished:




```
![[image (1).png]]

![[image.png]]

Duplicates dissapeared:

![[Pasted image 20220303172606.png]]

Lag 0 at the end:

![[Pasted image 20220303172644.png]]

Many "garbage topics" with many many topics:

![[Pasted image 20220303172832.png]]



---

--- 
### 17:30

### Status: #fleeting_note
### Tags:
[[schema_registry]]

### References

### Notes:

```ad-warning

before stop Connect stop replicator

```

- [x] Schema Registry to READONLY
- [x] Schema local backup
- [x] Stop Apps
- [x] Point Schema Registry to CCloud
- [x] Start Test App
- [x] Start Apps (KStreams included)

```ad-error

The script that stop the cluster, stop connect to before pausing replicator.

So we figure out how to delete it beofre start

We decide to just delete the connect config topic and then recreate the connectors again

Allianz face some problems on the connect config.


```

```ad-info

sanity check for streams and connect tomorrow.

```

---
