--- 
### 10:44

### Status: #fleeting_note
### Tags:
[[analisis_multivariante]] [[R]] [[principal_component_analysis]]
### References
[[An Introduction to Applied Multivariate Analysis with R]]
### Notes:

```ad-quote

***Principal Component Analysis*** is a *multivariate data exploratory technique* with the aim of reduce the dimensionality of a dataset while accounting for as much of the original variation as possible present on it.

```

```ad-danger
title: Curse of dimensionality

Having to many variables make impossible use graphical descriptive and other multivariant techniques.

```

```ad-summary
title: Core Math Concept

Transform a new set of **correlated variables:** $x^T = x_1,...,x_q$ to a smaller one of **uncorrelated variables** $y^T = y_1,..., y_q$ wich is a *linear combination* of x set.

$y_1$ accounts for as much as possible of the variation on X, then $y_2$ is chosen to account for as much as possible of the reamining one, subject to being uncorrlated with $y_1$ and so on.

$y_1,...y_q$ are the ***Principal Components***

```

```ad-example

A set of data consisting of examination scores for several different subjects for each of a number of students. One question of interest might be how best to construct an informative index of overall examination performance. One obvious possibility would be the mean score for each student, although if the possible or observed range of examination scores varied from subject to subject, it might be more sensible to weight the scores in some way before calculating the average, or alternatively standardise the results for the separate examinations before attempting to combine them. In this way, it might be possible to spread the students out further and so obtain a better ranking. The same result could often be achieved by applying principal components to the observed examination results and using the student’s scores on the first principal components to provide a measure of examination success that maximally discriminates between them.

```

```ad-summary
title: Finding Sample First Principal Component

$y_1 = a_11 x_1 + a_12 x_2 + · · · + a_1q x_q$

whose sample variance is greatest among all such linear combinations.

Variance of $y_1$ could be increased without limit simply by increasing the coefficients:

$a_1^T = (a_11, a_12,...a_1q)$

**where the sum of the squares of the coefficients should be 1**

$a_1$ is the vector that maximise the variance of $y_1$ subject to the *sum of squares constraint*

$a_1^T a_1 = 1$

The sample variance of $y_1$ is linear function of the x variables given by $a_1^T S_{a_1}$ where S is the $q \times q$ sample covariance matrix of x variables.

*Lagrange Multipliers* method is used for maximise the function so leads to the solution that $a_1$ is the *eigen or characteristic vector* of S, corresponding with the lagerst *eigen value or characteristic root*.

On $q \times q$ matrix A the *eigen values* $\lambda$ and the *eigen vectors* $\gamma$ are such that: $A_{\gamma} = {\lambda}_{\gamma}$ 

```

```ad-summary
title: Subsequent Components

The second principal component, y2 , is defined to be the linear combination

$y_2 = a_21 x_1 + a_22 x_2 + · · · + a_2q x_q$

(i.e., $y_2 = a_2^T x$, where $a_2 = (a_21 , a_22 , . . . , a_2q )$ and $x = (x_1 , x_2 , . . . , x_q )$) that has the greatest variance subject to the following two conditions:
$a_2^T a_2 = 1$,
$a_2^T a_1 = 0$.
(The second condition ensures that y1 and y2 are uncorrelated; i.e., that the sample correlation is zero.)

Similarly, the jth principal component is that linear combination $y_j = a_j^T x$
that has the greatest sample variance subject to the conditions
$a_j^T aj = 1$,
$a_j^T a_i = 0 (i < j)$.

Application of the Lagrange multiplier technique demonstrates that the vector
of coefficients defining the jth principal component, $a_j$ , is the **eigenvector of S
associated with its jth largest eigenvalue**.

```
```ad-note
title: First m Principal Components

The total variance
of the q principal components will equal the total variance of the original
variables so that:

$\sum_{i=1}^q {\lambda}_1 = s_1^2 + s_2^2 + ... + s_q^2$

or using [[trace_function| R Trace Function]]

$\sum_{i=1}^q {\lambda}_1 = trace(S)$

The jth principal component accounts for the proportion $P_j$:

$P_j = \frac{{\lambda}_j}{trace(S)}$

and **first m principal componentes**, where $m < q$:

$P^{(m)} = \frac{\sum_{j=1}^m {\lambda}_j}{trace(S)}$
```

```ad-note
title: Graphical Explanation

**First Principal Compoent** define the line of best fit to the q-observations in the sample. That means that this line minise the ortogonal residuals to the line.

The q-observations must be projected to the line.

On the rare case of observing collinearity on the q dimensions *covariance matrix* would have just one non-zero eigenvlaue.

The same way m first components fith in m dimensions and would be indicated by $q - m$ non-zero values on the *covariance matrix* that would imply $q - m$ *linear relationships between variables* that's called *Structural Relationships*

**In practise the majority of applications of principal component analysis will present all the eigenvalues of the covariance matrix will be non-zero**

```

```ad-note
title: Correlation vs Covariance Matrix

[[An Introduction to Applied Multivariate Analysis with R#^1749b8]]

[[princomp_funcion]]

[[Multivariate Descriptive Statisitics]]


*Principal Component Analysis* has hard dependency on the scale of the measures of the involved variables. So changing the measure units can have a huge impact on the results.

We can choose to do it with the "standard" *Covariance Matrix* or using R *Correlation Matrix*

**We should note that the result of both rarely coincide**

What we should use?:

Using *Covariance Matrix* implies a very critical danger of measuring the *order of size of the variance of the observed variables*

Using *Correlation Matrix* is equivalent to calculate the *Principal Components* as the *egigenvectors* of R that is the same of calculating it from the  original variables after each has been standarised to have a unit variance.

**So as general rule Principal Component Analysis should be performed on the Correlation Matrix **

```
```ad-info
title: Choosing Number of Components

1. Retain enough components to explain some large percetange of the orginal variation (~ 70-90%)
2. Exclude PC with eigen values less than average of observed variable: $\sum{i=1}^q = \frac{{\lambda}_i}{q}$
3. When components are extracted from Correlation Matrix, trace(R)=1 (average variance = 1) -> **exclude components with eigenvalues < 0.7**
4. **Scree Diagram** (${\lambda}_i$ against i) -> the point where curve changes from steep to shalow  is taken as last compoenent
5. variate of **scree diagram** with **log-eigenvalue diagram** ($log{\kambda}_i$ against i)

```

---