##### status: #daily_note 

# Tasks

#todo 
- [x] Check Helm Charts

# Work Notes

# Meeting Notes

# 2022-03-02 08:30 - 17:00
##### status: #meeting_note
##### Tags: [[replicator]] [[ccloud_migration]]

### Client:
[[Allianz]]
### Topic:
PREPROD MSK Migration
### Attendees:
* Nakul Mishra
* Oscar Gómez
* Marcus Barcelos
### Notes:

--- 
### 8:30

```ad-error
title: Monitoring

We tried to put in place monitoring with the dashboards we already provide (see Replicator Monitoring Guide).

We are not able to do so because Connect Instance are not exposing metrics.

```

```ad-error
title: Operator

We try to install through CFK Operator, but we don't have the certificates to setup properly the connections, so we avoid it.

We gonna check their Helm charts

```
Helm Chart Provided:

~~~yaml
aws:
  rolearn: 695964746098:role/oidc-role-kafka-connect

openshift: false
stage: "preprod"
dockerImage: "693773657855.dkr.ecr.eu-central-1.amazonaws.com/azdir/kafka-connect"
deployDomain: "allianzdirect-k8s-preprod.ec1.aws.aztec.cloud.allianz"

replicaCount: 2
kafkaOptions: "-Djava.security.auth.login.config=/etc/kafka-connect/basic-auth/config/KafkaConnectJAAS"
connectDebug: "y"

overrideGroupId: "kafka-connect-preprod"

configurationOverridesMsk:
  "plugin.path": "/usr/share/java,/usr/share/confluent-hub-components,/etc,/usr/share/confluent-hub-components/confluentinc-kafka-connect-replicator/lib/*"
  "key.converter": "org.apache.kafka.connect.storage.StringConverter"
  "key.converter.schemas.enable": false
  "value.converter": "io.confluent.connect.avro.AvroConverter"
  "value.converter.schemas.enable": true
  "config.storage.replication.factor": "3"
  "offset.storage.replication.factor": "3"
  "status.storage.replication.factor": "3"
  "security.protocol": "SSL"
  "consumer.security.protocol": "SSL"
  "consumer.key.deserializer": "org.apache.kafka.common.serialization.ByteArrayDeserializer"
  "consumer.value.deserializer": "org.apache.kafka.common.serialization.ByteArrayDeserializer"
  "producer.security.protocol": "SSL"
  "producer.key.serializer": "org.apache.kafka.common.serialization.ByteArraySerializer"
  "producer.value.serializer": "org.apache.kafka.common.serialization.ByteArraySerializer"
  "config.providers": "file"
  "config.providers.file.class": "org.apache.kafka.common.config.provider.FileConfigProvider"
  "rest.extension.classes" : "org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension,io.confluent.connect.replicator.monitoring.ReplicatorMonitoringExtension"
  "rest.advertised.host.name" : "kafka-connect.allianzdirect-k8s-preprod.ec1.aws.aztec.cloud.allianz"
~~~

```ad-success

Helm chart checked and figure out they are missing the prometheus exporter sidecar part.

~~~yaml
prometheus:
  ## JMX Exporter Configuration
  ## ref: [https://github.com/prometheus/jmx_exporter](https://github.com/prometheus/jmx_exporter)
  jmx:
    enabled: true
    image: solsson/kafka-prometheus-jmx-exporter@sha256
    imageTag: 6f82e2b0464f50da8104acd7363fb9b995001ddff77d248379f8788e78946143
    imagePullPolicy: IfNotPresent
    port: 5556

    ## Resources configuration for the JMX exporter container.
    ## See the `resources` documentation above for details.
    resources: {}
~~~

Working on a my cluster. 

Provided to Marcus that gonna put in place by the end of the day (will be effective tomorrow)

```


---

--- 
### 10:30

```ad-info

1 Connect cluster deployed (2 pods)

Replicator Connector deployed on it.

~~~json
{
  "name": "replicator-3",
  "config": {
    "connector.class": "io.confluent.connect.replicator.ReplicatorSourceConnector",
    "dest.kafka.sasl.mechanism": "PLAIN",
    "dest.kafka.ssl.endpoint.identification.algorithm": "https",
    "src.kafka.ssl.key.password": "PASSWORD",
    "topic.whitelist": <bunch of topics>,
    "offset.translator.tasks.max": "0",
    "producer.override.security.protocol": "SASL_SSL",
    "dest.topic.replication.factor": "3",
    "confluent.license": "${file:/etc/kafka-connect/secrets/connectors_passwords.properties:confluent-license}",
    "confluent.topic.ssl.keystore.location": "${file:/etc/kafka-connect/kafka-connect.properties:ssl.keystore.location}",
    "dest.kafka.security.protocol": "SASL_SSL",
    "src.kafka.security.protocol": "SSL",
    "topic.config.sync": "true",
    "confluent.topic.ssl.keystore.type": "PKCS12",
    "dest.kafka.retry.backoff.ms": "500",
    "confluent.topic.replication.factor": "3",
    "producer.override.request.timeout.ms": "20000",
    "src.kafka.ssl.keystore.location": "/etc/kafka-connect/cert/keystore.jks",
    "value.converter": "io.confluent.connect.replicator.util.ByteArrayConverter",
    "key.converter": "io.confluent.connect.replicator.util.ByteArrayConverter",
    "producer.override.retry.backoff.ms": "500",
    "offset.timestamps.commit": "false",
    "producer.override.bootstrap.servers": "lkc-m30yw-lg1x1.eu-central-1.aws.glb.confluent.cloud:9092",
    "confluent.topic.bootstrap.servers": "${file:/etc/kafka-connect/kafka-connect.properties:bootstrap.servers}",
    "dest.kafka.request.timeout.ms": "20000",
    "header.converter": "io.confluent.connect.replicator.util.ByteArrayConverter",
    "src.kafka.sasl.mechanism": "PLAIN",
    "confluent.topic.ssl.keystore.password": "${file:/etc/kafka-connect/kafka-connect.properties:ssl.key.password}",
    "src.consumer.group.id": "replicator-2",
    "dest.kafka.bootstrap.servers": "lkc-m30yw-lg1x1.eu-central-1.aws.glb.confluent.cloud:9092",
    "confluent.topic.security.protocol": "${file:/etc/kafka-connect/kafka-connect.properties:security.protocol}",
    "name": "replicator-3",
    "producer.override.sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"I4VRUB75JCZ37G4T\" password=\"PASSWORD\";",
    "topic.auto.create": "true",
    "producer.override.sasl.mechanism": "PLAIN",
    "provenance.header.enable": "true",
    "dest.kafka.sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"I4VRUB75JCZ37G4T\" password=\"PASSWORD\";",
    "producer.override.ssl.endpoint.identification.algorithm": "https",
    "src.kafka.bootstrap.servers": "SSL://b-1.kafka-pre-prod.0djbl4.c4.kafka.eu-central-1.amazonaws.com:9094,SSL://b-2.kafka-pre-prod.0djbl4.c4.kafka.eu-central-1.amazonaws.com:9094,SSL://b-3.kafka-pre-prod.0djbl4.c4.kafka.eu-central-1.amazonaws.com:9094,SSL://b-4.kafka-pre-prod.0djbl4.c4.kafka.eu-central-1.amazonaws.com:9094,SSL://b-5.kafka-pre-prod.0djbl4.c4.kafka.eu-central-1.amazonaws.com:9094,SSL://b-6.kafka-pre-prod.0djbl4.c4.kafka.eu-central-1.amazonaws.com:9094",
    "src.kafka.ssl.keystore.password": "PASSWORD"
}
~~~

**700 Mill Messages** over **3k+ Partitions**

All topics migrated except:  optr*, __consumer_offset, __amazon, __mm2, __connect_offset, __connect_status


```

```ad-warning

Too many work for 1 instance of replicator, on top of that we have just 1 task configured.

We recommend the approach of having more instances of Replicator (on specific cluster) with specific resources for a group of topics. This will improve:

1. Resilience: In case of failure we don't have to figure out what was wrong on 3k partitions just in a few ones beside of the RTO on that case.
2. Throughput will increase massively

```


```ad-success
title: Replicator Test

Test topic replication without problem:

**

Input-topic : migration-march-1(MSK)

Output:  migration-march-1 (CCloud)

Actual : 10_000

Expected: 10_000

  

Push : 100_000

Expected: 110_000 (100_000 + prev 10_000)

**
```

---

````ad-summary
title: Topic Migration


Migration Starts with 1 Task so as expected we moving slow (not quantified)

Decide to move to 5 task and 5 pods. **Still Slow**

Some disccusion happens about [task rebalancing](https://docs.confluent.io/platform/current/connect/concepts.html#task-rebalancing).

On this point marcus decide to kill some pods and just experiment what happens with the rebalancing.

We decide to move to **30 task with 30 pods**. Reabalancing its happenning on the process.

Now we got a decent thorughput

````

```ad-success


Migration Completed:

**

MSK cluster : 700Million+ messages

CCloud : 40Million messages

**

Looking into the skip topics we realize that just \_consumer\_offset have more than half million. SO the numbers match

```
```ad-warning

We found some duplicates on the migrated topics (at leat 1 serious one with 6x more messages). We can explain that due the killing pods (like a kill -9), rebalancing, network issues.

But on top of that we emphatize the fact of not having this kind of behaivour on a distributed system is almost impossible, and the fact of the *at least one* semantic as default one in Kafka.

```

```ad-summary
title: Test App Migration

We choose a *Spring Cloud App* (producer and consumer in one) to check the migration

We go without any preparation just change bootstrap endpoints and credential on al K8s Map and restart.

```

```ad-error

App is able to produce but not to consume.

We found a error on consumer group permission.

Other environments seems to work properly

```

```ad-hint

On other environments they are running with **user account** credentials that grant permission to al resources.

On PREPROD we run with a **service user** so we need to grant permission properly.

Marcus decide to create a new account user for prepod (**we don't recommend this for PROD**).

After that all working.

```

### Improvement Room and reminders

 #todo
- [ ] Provide kcat and ccloud cheatsheet we struggle a lot of time finding and sending to them the commands
- [ ] Discuss about topic naming convention (our life can me much more easier)
- [ ] Provide runbook about component install with [[cfk]] (I already have one). Push them back from deprecated helmcharts

### Day Recap 

- [x] topics fully migrated
- [x] ACLs put in place
- [x] Test Java App On migrated topic
 
### Next Steps

- [ ] Fresh Start on new Cluster$^1$
	- [ ] Provision new cluster
	- [ ] Create Accounts and new API Keys
- [ ] {Nice to Have} Monitoring on the Helm Chart
- [ ] Start Replicator with the agreed configuration
- [ ] Migrate Topics on the whitelist
- [ ] Migrate Producers/Consumers
- [ ] Migrate Kstream apps
- [ ] Migrate Connectors
- [ ] Cleanup

```ad-warning
title: 1

We Expect that Marcus will do some work in advance so tomorrow we can start from the topic migration or even from the Apps but still start from scratch on a new cluster leaves us with few time to complete all the tasks 

```
